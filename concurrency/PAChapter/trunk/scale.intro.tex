\section{Introduction}\label{sec:introduction} % (fold)

In his Turing award acceptance paper \cite{}, Milner explicitly
recognized an ``equation'' of the form:
\begin{figure}[hbp]
  \makebox[\width][s]{
    \begin{mathpar}
      \inferrule {\lambda\textit{-calculus}}{\textit{sequential computing}}
      =
      \inferrule {X}{\textit{concurrent computing}}
    \end{mathpar}
  }
\end{figure}
proposing that $\pi$-calculus was a solution for $X$, i.e. that the
$\pi$-calculus represented a clean, simple, compositional model of
\emph{concurrent} computation; a foundational model of concurrent
computation playing the same foundational role that the
$\lambda$-calculus played for sequential, functional computation.

His motivations for seeking such a model are not far from the
motivations of many engaged in today's industrial and commercial
computing sectors. At present we find something of a ``concurrency
crisis'' facing the industrial and commercial software development
sectors. Software development is under pressure from above and from
below. From above we have already witnessed the profound change the
commercialization of the Internet has had on the nature of the
commercial software application. With few exceptions, software
applications are now commercially uninteresting unless they are
multi-user and able to interoperate over a wide range of communications
protocols. From below, the end of the Moore's law ``free lunch'' is
causing hardware vendors to ``go sideways'' producing multicore
offerings for both consumer and enterprise markets. Taken together
these two trends mean that parallelism, concurrency and distribution
are the prevailing paradigms from the hardware up through the
end-user's expectations of availability and responsiveness.

Unfortunately, mainstream programming models \footnote{as embodied in
  mainstrema programming languages like \texttt{C++} and
  \texttt{Java}} have not kept up with the times, offering out-dated
and out-moded abstractions for describing and implementing parallel,
concurrent and distributed programs. Threads and locks, for example,
are notoriously difficult abstractions with which to program because
there is no syntactic support allowing the programmer -- or
programmatic analysis -- to check by \emph{syntactic} means important
notions like ``how many threads might be expected to run through this
code simultaneously?'' or ``what are the actual boundaries of this
critical section; what resources are guaranteed protection from
simultaneous access?''. Thus the commercial developer is faced with
developing systems of increasing complexity with little or no direct
support from the language in which she expresses her application
codes.

In response, the market has seen a huge uptick of models, frameworks
and solutions developed and promulgated to address the complexity of
developing commercial systems in this environment. A typical
commercial web application, for example, involving
\begin{itemize}
\item asynchronous \texttt{XML} requests through JavaScript
  (a.k.a. \texttt{AJAX}) \cite{}
\item a webserver engine (such as \texttt{Tomcat} or \texttt{Jetty}) \cite{}
\item a relational database (such as \texttt{MySQL}) or an XML
  database (such as \texttt{BDBXML}) (or both) \cite{}
\item access to a handful of Web Services \cite{}
\item and deployed over a grid \cite{}
\end{itemize}
will mix several different core concurrency models including both
synchronous and asynchronous message passing as well as
transaction-based models. Even when using a framework like
\texttt{RubyOnRails} \cite{} which provides end-to-end ``templates''
and conventions for common case applications, the developer is still
implicitly keeping these different models in her head as she develops,
tests and profiles for performance. While some of this
(infra-)structure actually represents optimization for widely adopted
usage patterns (such as consistent local storage that persists across
access) much of it is accidental.

Having an effective model of concurrent and distributed computation
that could successfully encompass the different styles of concurrent
execution found in today's commercial applications -- much in the way
that the $\lambda$-calculus was able to embrace different sequential
programming paradigms leading to epithets like ``lambda the ultimate
declarative'' or ``lambda the ultimate imperative'' or, eventually,
just ``lambda the ultimate'' \cite{} -- would serve many
purposes. First and foremost, it would provide some handle or leverage
on application complexity, much in the way that $\lambda$-abstraction
provided sorely needed abstraction over various program structure
details. The appeal of the $\lambda$-calculus has ever been that its
simplicity and abstraction enabled a view into the essential aspects
of program structure. Certainly, Milner and others working in the area
of concurrency theory have held hopes that one or more of the calculi
developed there would provide a similar vantage point for concurrent
applications. Such a vantage point, as we have seen with the
theoretical development of the $\lambda$-calculus, is also the grounds
on which we can hope to approach desirable goals, like correctness, or
automated program transformation; and, these goals have very practical
implications for robustness, availability and responsiveness. We
cannot resist also pointing out that automated program transformation
has seen a resurgence of commercial interest as witnessed in support
for so-called ``refactoring'' in industrial interactive development
environments.

Even as we applaud the virtues of the $\lambda$-calculus, however, we
need to recognize that there was a considerable effort over some 50
years to get from Church and co's original formulations to a family of
theories that can be seen to underlie and inform useful, performant
systems such as \texttt{OCaml}, or \texttt{Haskell} or \texttt{Scala},
or ... . Today, we have systems like Sewell's \texttt{Caml Light}
\cite{}, effectively \emph{mechanically formalizing} the theory that
encompasses the lion's share of the operational semantics of the
\texttt{OCaml} language, but the language design communities have not
always seen formalisms like the $\lambda$-calculi as much more than
approximate descriptions -- toy models -- of real programming
languages and execution engines. There was a two-way dialogue between
theory and practice, taking place over several decades, to develop a
sufficiently detailed formal account to even contemplate something
like \texttt{OCaml Light} -- though many dreamed of it. Thus, even if
Milner's $\pi$-calculus, or some family relative, does embody a
$\lambda$-like, foundational model of concurrent computation, getting
from there to a theory that can fruitfully underpin the efforts of the
commercial web application developer is likely to be a similar effort
as the passage from $\lambda$-calculus to high-performance functional
programming languages and execution engines.

Following Milner's lead, then, this chapter explores desiderata for
solutions to the following ``equation'':

\begin{figure}[hbp]
  \mbox{
    \begin{mathpar}
      \inferrule {\lambda\textit{-calculus}} {\{ \texttt{Haskell}, \texttt{OCaml},
        \texttt{Scala} \}}
      =
      \inferrule {\pi\textit{-calculus}} {X}
    \end{mathpar}
  }
\end{figure}

\subsection{20-20 hindsight}

We seek to take advantage of our hindsight perspective on the
development from the $\lambda$-calculus to high-performance functional
systems. We focus on the introduction of rich data structuring
techniques, noting that the original formulations of both calculi lack
such techniques. In the case of the lambda calculus, as we argue
below, on the one hand, it was necessary to incorporate a rich account
of data to obtain a level of utility where it could be seen as
informing the design and semantics of something like \texttt{Haskell}
or \texttt{OCaml}, while on the other, it was a non-trivial task to
reconcile $\lambda$'s wholly operational view of data with an account
that could connect a wide range of developers with a wide range of
application tasks.

The suggestion is that it is not egregiously revisionist to view the
history of the development of functional theory and practice in terms
of a sequence of transformations of $\lambda$-calculus. Each
transformation yields a calculus with a better, more fine-grained
account of programming practice, simultaneously presenting proposals
for refinements of programming practice. Further, the idea is to focus
primarily on those transformations that have had to do with providing
richer accounts of data. Data and control are tightly interwoven, in
general, but more so in languages derived from an underlying algebraic
design, like the $\lambda$ and $\pi$-calculi. Thus, such a focus is
not really limiting, in our view, but primarily a mechanism to scope
and frame the investigation.

In terms of our ``equational'' characterization, we are suggesting
that the calculi underlying languages like \texttt{Haskell} or
\texttt{OCaml} look like $T_0(T_1(\ldots T_n(\lambda)\ldots ))$. We
are positing that with a focus on those transformations associated
with providing better accounts of data we find candidates for
transformations to apply to the $\pi$-calculus to get better and
better accounts of a viable concurrent programming practice.

\begin{figure}[hbp]
  \mbox{
    \begin{mathpar}
      \inferrule {T_0(T_1(\ldots T_n(\lambda\textit{-calculus})\ldots ))}
      {\{ \texttt{Haskell}, \texttt{OCaml}, \texttt{Scala} \}} =
      \inferrule {T_0(T_1(\ldots T_n(\pi\textit{-calculus})\ldots ))} {X}
    \end{mathpar}
  }
\end{figure}

At a qualitative level, we are putting forward two mutually supporting
positions regarding desiderata in language design that bear on the
introduction of data and control structuring techniques.

\paragraph{Data-control coherence}
Firstly, one of the key facets of a successful language design is the
degree to which data structuring techniques and control structuring
techniques \emph{cohere}. On the one hand, the enthusiastic
exploration of object-oriented language design proposals can be
understood as driven by an underlying desire to find mediating
structures between data and control, to find ways to make them cohere,
regardless of the ultimate success or failure of the techniques
embraced by the programme. On the other, the proposals painstakingly
driven by the interaction of practice and theory -- a much slower,
more rigorous and demanding community process -- resulting in the
modern functional language designs, like \texttt{ML} or
\texttt{Haskell}, are noted for their coherence -- which can be
measured, to some degree in their simplicity and opportunity for
abstraction. Examples include

\begin{itemize}
\item the happy alignment of type constructs like summation and tupling
  with pattern-matching
\item parametric polymorphism (a.k.a. generics), resulting in dual
  abstraction across data-structuring and algorithms exerting control
  flow through these patterns
\item monads, by abstracting the notion of composition, provide
  generalized mechanisms for contructing and destructuring containers,
  hence Wadler's observations of monads as providing generalized
  notions of comprehension \cite{}, which have been widely adopted in
  industry, even beyond mainstream functional languages.
\end{itemize}

\paragraph{Reflection}
Secondly, we argue that an alternative measure of this coherence is
found in meta-programming, and more specifically reflective
capabilities. The ease by which program structure, including control
constructs, may be treated programmatically as data, and conversely,
the ease by which such programmatically generated data may be deployed
and executed, provides for a practical test of this coherence
property. This measure is akin to the first practical test of a
compiler: bootstrapping \cite{}. Similarly, the experiences of users
of an earlier generation of functional languages, such as
\texttt{Lisp} or \texttt{Scheme} on the ease of converting back and
forth between \texttt{S}-expressions and executable code gave them a
sense of that kind of coherence and seeded the development of numerous
applications of meta-programming \cite{}. Ultimately, a variety of
practical requirements of large-scale development have driven support
for reflection-based meta programming into mainstream programming
languages such as \texttt{Java} or Microsoft's \texttt{.net}
languages. We stress that meta-programming techniques are essential in
large-scale development because -- even when developers are enabled by
good abstractions -- the programs are often too large to be written by
hand; developers must employ programs to write programs \footnote{a
  good example is the commonly accepted practice of generating the map
  from in-memory representations of data structures to their backing
  store -- a.k.a. the object-relational mapping}, and in many cases it
is more effective to do this dynamically \footnote{cf. the popular
  Spring framework}, i.e. to generate programs during runtime.

Interestingly, reflection-based meta-programing is yet to appear in
statically typed mainstream functional languages. There are noteworthy
developments, such as \texttt{MetaOCaml} \cite{}, and \texttt{Template
  Haskell} \cite{} can be seen as a recognition of some of the basic
requirements of meta-programming \cite{}, but as yet the natural
combination of rich, but strongly and statically typed data constructs
and monadic structuring techniques, on the one hand with reflective
capabilities, on the other has not come about in any of the mature
functional platforms. This may be due, in part, from the historical
accident that functional language development bifurcated along
so-called ``static'' and ``dynamic'' lines, with the former line
proceeding almost directly from \texttt{ML} and the latter proceeding
more or less directly from \texttt{Lisp}. It has taken the various
communities some time to understand the value of the feature sets
explored in ``isolation'' and will take still more time to integrate
these notions. It may also be due to a lack of theoretical treatment
of reflection. Again, there are a handful of efforts \cite{}, but none
aimed at industrial scale on the one hand, or foundations on the
other.

Either way, we are arguing that the lessons learned in developing rich
data structuring techniques, and reflective meta-programming
techniques for functional programming should be applied to the
development of concurrent programming. Much of what has gone on in the
passage from the $\lambda$-calculus to modern functional programming
has been about development of feature proposals that can be seen in
terms of a succession of natural, logical transformations from the
core theory ($\lambda$-calculus) to a theoretical account informing
and describing the practical features. The development of an
industrial strength concurrent language could do worse than to follow
this model. A good deal of evidence has been gathered since Milner's
Turing award acceptance paper that the $\pi$-calculus is an excellent
candidate as an analog of the $\lambda$-calculus in this development
model.

% section introduction (end)